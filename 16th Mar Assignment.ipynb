{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dedc14-135c-4c9a-8229-61fc83a6427f",
   "metadata": {},
   "source": [
    "# 16th March Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e97d79-2cd8-43fa-8c9b-a3fa92117b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3cb9e91-4564-4e07-82c3-f3948ce06d3a",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7760b2b-e9a9-48db-9e45-484691c4a9a7",
   "metadata": {},
   "source": [
    "* Overfitting\n",
    "\n",
    "-> When machine learning model performs very well in training,but doesn't perform that much well in testing so we call it overfitting.\n",
    "\n",
    "-> The consequences of overfitting is low accuracy of model and poor performance.\n",
    "\n",
    "* Underfitting\n",
    "\n",
    "-> When machine learning model doesn't performs very well in training,and also perform bad at testing that is call underfitting.\n",
    "\n",
    "->T he consequences of underfitting is high error rate on training and testing data.\n",
    "\n",
    "\n",
    "* This approaches are usufull for mitigate overfitting\n",
    "\n",
    "Increase training data\n",
    "\n",
    "Simplify the model\n",
    "\n",
    "Regularization\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "* This approaches are usufull for mitigate overfitting\n",
    "\n",
    "Increase model complexity\n",
    "\n",
    "Feature engineering\n",
    "\n",
    "Try different algorithms\n",
    "\n",
    "Reduce regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea09dcb-1191-4d9d-a848-4f1c182580b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83e77406-b948-4cb7-8bb1-7ebd165326d8",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ab5b4-17f4-47e0-8489-44eaccfbf86b",
   "metadata": {},
   "source": [
    "* We can apply this techniques to reduce overfitting\n",
    "\n",
    "(i) Increase training data: With a larger and more diverse dataset, the model has a better chance of learning the underlying patterns and generalizing well to new instances.\n",
    "\n",
    "(ii) Simplify the model:Simplifying the model by reducing its complexity can help prevent it from memorizing noise or irrelevant details in the training data.\n",
    "\n",
    "(iii) Cross-validation: Cross-validation is a technique used to assess the model's performance on multiple subsets of the data. It helps estimate how well the model will generalize to new instances. \n",
    "\n",
    "(iv) Early stopping: Early stopping involves stopping the training process when the model's performance on the validation set starts to deteriorate. This prevents the model from continuing to learn noise in the training data.\n",
    "\n",
    "(v) Regularization: Regularization techniques introduce additional constraints on the model to prevent overfitting. Two common regularization methods are L1 regularization (Lasso) and L2 regularization (Ridge). These techniques add penalty terms to the loss function, discouraging large or complex parameter values and promoting simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a367b-9f9b-402a-bd37-2f5a2e9751f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "986ee827-05f9-41af-ac6a-3e2bac696c7c",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafb0a4-efcc-4b15-91d3-3a486bb8590d",
   "metadata": {},
   "source": [
    "-> It occurs when the model fails to learn from the training data adequately and performs poorly not only on the training data but also on new, unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de0d85-1c69-423f-b9e5-b7d20aa31def",
   "metadata": {},
   "source": [
    "* Scenarios where underfitting can occur\n",
    "\n",
    "(i) When a linear model is used to fit a nonlinear relationship between variables.\n",
    "\n",
    "(ii)When the dataset is small or lacks diversity, preventing the model from learning the full range of patterns.\n",
    "\n",
    "(iii) When important features are not included or relevant information is overlooked during feature engineering.\n",
    "\n",
    "(vi) When the regularization strength is excessively high, constraining the model too much.\n",
    "\n",
    "(v)When a simple algorithm is chosen for a task that requires more powerful algorithms to capture complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5d7b6-b7f1-4f70-be74-1e9e0826af39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13338c6a-6aca-48cc-aa03-5bee96acae2c",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f2641-ddfc-46ad-8112-c9260612ca2a",
   "metadata": {},
   "source": [
    "* The bias-variance tradeoff arises from the inherent tradeoff between bias and variance in model performance. As the model's complexity increases, its ability to fit the training data improves, resulting in lower bias. However, this also increases the model's sensitivity to the training data, leading to higher variance. Conversely, reducing the model's complexity decreases its sensitivity to the training data, resulting in lower variance but higher bias.\n",
    "\n",
    "* Relationship between bias and variance\n",
    "\n",
    "(i) High Bias and Low Varriance\n",
    "\n",
    "-> High bias means that model performed poor while training the model.\n",
    "\n",
    "-> Low varriance means model performed very well during the test of model.\n",
    "\n",
    "(ii) Low Bias and High Varriance\n",
    "\n",
    "-> Model performe excellent when bias is low.\n",
    "\n",
    "-> Model performe poor when varriance is high.\n",
    "\n",
    "* The aim is to minimize the total error, which is the sum of the bias squared and the variance. This can be achieved through techniques such as regularization, cross-validation, and model selection, which help strike the right balance between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacd18c-384c-44ec-a8f5-623dd0005dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3823c85e-9ae0-4d3e-b5fc-5aa863f321b6",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af962361-dd4a-4bdc-a825-f05421a6a16c",
   "metadata": {},
   "source": [
    "*  Methods for detecting overfitting and underfitting in machine learning models\n",
    "\n",
    "Model complexity\n",
    "\n",
    "Regularization effects\n",
    "\n",
    "Evaluation on training and validation sets\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "Residual analysis\n",
    "\n",
    "Learning curves\n",
    "\n",
    "-> Regular monitoring and evaluation of the model's performance throughout the training and validation process are crucial for detecting and addressing overfitting and underfitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd584fe-f59d-4138-932e-18a180e87dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61f811f1-3d14-4662-9680-1e99716ea5df",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69a63f-a3da-468c-ad61-1a1b1c61ca5b",
   "metadata": {},
   "source": [
    "Bias:\n",
    "\n",
    "Bias refers to the error introduced by the model's assumptions or simplifications.\n",
    "It represents the model's tendency to underfit or oversimplify the underlying patterns in the data.\n",
    "High bias models have low complexity and make strong assumptions about the data.\n",
    "These models may overlook important details and have systematic errors.\n",
    "High bias models may struggle to capture complex relationships and tend to have poor performance on both training and test data.\n",
    "\n",
    "Variance:\n",
    "\n",
    "Variance refers to the variability of the model's predictions for different training datasets.\n",
    "It represents the model's sensitivity to the specific training data.\n",
    "High variance models have high complexity and are prone to overfitting.\n",
    "These models are flexible and can capture complex patterns, including noise and random fluctuations in the training data.\n",
    "High variance models may perform well on the training data but generalize poorly to new, unseen instances.\n",
    "\n",
    "* Example\n",
    "\n",
    "Bias : A linear regression model used to predict a highly nonlinear relationship between variables.\n",
    "\n",
    "Varriance : A deep neural network with a large number of layers and parameters that fits the training data very closely.\n",
    "\n",
    "\n",
    "* Differences\n",
    "\n",
    "\n",
    "-> High bias models:\n",
    "\n",
    "High training error: They fail to capture the underlying patterns and exhibit systematic errors, resulting in high error rates on the training data.\n",
    "\n",
    "High test error: Due to oversimplification, they also perform poorly on unseen data, resulting in high error rates on the test data.\n",
    "\n",
    "Underfitting: High bias models are typically associated with underfitting, where the model is not able to capture the complexity of the problem.\n",
    "\n",
    "-> High variance models:\n",
    "\n",
    "Low training error: They have the potential to fit the training data very closely, resulting in low error rates on the training data.\n",
    "High test error: However, due to overfitting and capturing noise or random fluctuations, they fail to generalize well, leading to high error rates on the test data.\n",
    "\n",
    "Sensitivity to training data: High variance models are highly sensitive to the specific training data and may produce different predictions for different datasets.\n",
    "\n",
    "Overfitting: High variance models are associated with overfitting, where the model becomes too specific to the training data and fails to generalize to new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d28f1-e0e8-4656-ba3a-5e0047ec428a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76be9460-4509-4392-95e5-190b4d0496db",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aeb6ef-4d01-4405-9bb9-f9dfff20d175",
   "metadata": {},
   "source": [
    "-> Regularization is a technique used in machine learning to prevent overfitting by adding additional constraints or penalties to the model during the training process. It helps in controlling the model's complexity and discourages it from fitting noise or irrelevant details in the training data.\n",
    "\n",
    "-> The main idea behind regularization is to introduce a tradeoff between minimizing the training error and reducing the complexity of the model. By adding regularization, the model is encouraged to find a balance between fitting the training data well and generalizing to new, unseen data.\n",
    "\n",
    "* Techniques\n",
    "\n",
    "L1 Regularization (Lasso): L1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the model's coefficients. It encourages sparsity by driving some of the coefficients to exactly zero. This results in feature selection, where less important features are automatically assigned zero coefficients. L1 regularization helps in feature selection and simplifies the model.\n",
    "\n",
    "L2 Regularization (Ridge): L2 regularization adds a penalty term to the loss function that is proportional to the square of the model's coefficients. It encourages the model to have smaller and more distributed coefficient values. L2 regularization helps in reducing the impact of individual features and smoothens the model's output.\n",
    "\n",
    "* How they Work\n",
    "Elastic Net Regularization: Elastic Net regularization combines both L1 and L2 regularization. It adds a penalty term that is a linear combination of the L1 and L2 norms of the model's coefficients. The Elastic Net regularization allows for feature selection like L1 regularization while also addressing the limitations of L1 regularization when faced with highly correlated features.\n",
    "\n",
    "Dropout: Dropout is a regularization technique specifically used in neural networks. It randomly sets a fraction of the neurons' outputs to zero during each training iteration. This prevents specific neurons from over-relying on certain inputs or co-adapting with other neurons. Dropout acts as a form of model averaging and helps in reducing overfitting by improving the model's generalization.\n",
    "\n",
    "Early Stopping: Early stopping is not a traditional regularization technique but can be used to prevent overfitting. It involves monitoring the model's performance on a separate validation set during the training process. Training is stopped when the validation performance starts to deteriorate. Early stopping helps to find the point where the model has learned enough from the training data and prevents it from overfitting by avoiding further training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba1a4e-c10c-49c8-bf38-88fd721bd364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
